<html>
    <head>
        <style>
            #wizzy { width:300px; float:left; margin-right:10px}
            #chat { width:700px; float:left; margin-right:10px}
            #robotics {
                grid-area: sec1;
                color: white;
                background-color: #921613;
            }
            #conversation {
                grid-area: sec2;
                color: white;
                background-color: #171A40;
            }
            main {
                display: grid;
                grid: 'sec1' 'sec2';
            }
            ul { overflow: hidden; }
            .hidden { 
                visibility: hidden;
                display: none;
            }
            h1, h4, p, ul {
                font-family:sans-serif;
                font-size: 30px;
                margin-left:10px;
            }
        </style>
    </head>
    <body>
        <main>
            <section id="robotics">
                <img id="wizzy" src="images/wizzy.jpg">
                <h1>Assistive Robotics</h1>
                <h4>Shared control between users and smart wheelchairs</h4>
                <p>
                    The Wizzybug is a powered wheelchair for children made by Bath based charity, Designability.
                    We are investigating how the use of powered wheelchairs can be extended to include children with motor difficulties that hamper their ability to accurately steer the Wizzybug.
                    If we can gain enough information about the child's goal through a noisy control signal input via the joystick, then the computer can augment the control signal to support the child in achieving their goals.
                    We don't want to make the child a passenger in their own wheelchair. 
                    The aim is to avoid the computer taking over, through the idea of <em>shared control</em>.
                </p>
                <p>
                    Our work is inspired by a branch of cybernetics called <em>Perceptual Control Theory</em>.
                </p>
                <p>Collaborations with Kaya Sinclair, Praminda Caleb-Solly, Dave Meckin:</p>
                <ul>
                    <li>Robopsychology: Perceptual Control Theory and Braitenberg's vehicles (<a href="https://www.youtube.com/watch?v=qiwSli7eiS4&t=3069s">video</a>)</li>
                    <li>Robots with Purpose (<a href="https://scholar.google.co.uk/citations?view_op=view_citation&hl=en&user=k3bLaFAAAAAJ&cstart=20&pagesize=80&citation_for_view=k3bLaFAAAAAJ:RGFaLdJalmkC">article</a>)</li>
                </ul>
            </section>
            <p>
            <section id="conversation">
                <img id="chat" src="images/chat.png">
                <h1>Conversational Interfaces</h1>
                <h4>Goal-driven chatbots and task-oriented dialogues</h4>
                <p>
                    The example dialogue (pictured) shows a conversational interface designed to answer questions about ongoing regeneration projects in Keynsham. 
                    This research was done in conjunction with Keynsham Town Council, and this simple FAQ chatbot answers questions about the High-Street Action Zone Project, providing complex information that is hard to organise on a conventional website.
                    It is designed to identify the user <em>intent</em> and consult a knowledge base for suitable facts that it uses in its answers.
                </p>
                <p>Seen through cybernetic specs, the goal of the chatbot is to move the conversation towards a satisfactory conclusion.
                </p>
                <p>Collaborations with Camilo Pires, Nathan Duran, Ryan Fellows, Jim Smith, Hisham Ihshaish:</p>
                <ul>
                    <li>ChatBot integration with Knowledge Graph technology (<a href="FETEnterpriseProjects2020-2021.pdf">FET Enterprise Projects 2020-2021</a>)</li>
                    <li>Inter-annotator Agreement Using the Conversation Analysis Modelling Schema, for Dialogue (<a href="https://www.researchgate.net/publication/357895917_Inter-annotator_Agreement_Using_the_Conversation_Analysis_Modelling_Schema_for_Dialogue">article</a>)</li>
                    <li>Conversation Analysis Structured Dialogue for Multi-Domain Dialogue Management (<a href="https://www.researchgate.net/publication/329809503_Conversation_Analysis_Structured_Dialogue_for_Multi-Domain_Dialogue_Management">paper</a>)</li>
                    <li>Sentence encoding for Dialogue Act classification (<a href="https://www.researchgate.net/publication/355858479_Sentence_encoding_for_Dialogue_Act_classification">article</a>>)</li>
                    <li>Task-oriented Dialogue Systems: performance vs. quality-optima, a review (<a href="https://www.researchgate.net/publication/357239121_Task-oriented_Dialogue_Systems_performance_vs_quality-optima_a_review">preprint</a>)</li>
                    <li>Understanding Learning Strategy as Conversation (<a href="https://www.researchgate.net/publication/324898015_Understanding_Learning_Strategy_as_Conversation">paper</a>)</li>
                </ul>
            </section>
            <p>The background image used in the poster is <a href="https://www.google.co.uk/books/edition/Beyond_Art_A_Third_Culture/xkk6U42Zl_sC?hl=en&gbpv=1&dq=%27Two+Circles%27+by+Anna+BÃ©othy-Steiner&pg=PA55&printsec=frontcover">'Two Circles' by Anna B&eacute;othy-Steiner (1930)</a></p>
        </main>
        <script>
            hash = window.location.hash;
            if (hash) {
                hash = hash.substring(1);
                sections = document.getElementsByTagName('section');
                for (i=0; i<sections.length; i++) {
                    if (sections[i].id!=hash) {
                        sections[i].classList.add("hidden");
                    }
                }
            }
        </script>
    </body>
</html>